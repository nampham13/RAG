# RAG System Configuration
paths:
  data_dir: "data"
  prompt_path: "prompts"

ui:
  default_backend: "gemini"
  default_local_backend: "ollama"  # Chọn "ollama" hoặc "lmstudio"

llm:
  gemini:
    # API key is read from .streamlit/secrets.toml
    model: "gemini-2.0-flash"
    temperature: 0.7
    max_tokens: 2048

  ollama:
    base_url: "http://localhost:11434"
    model: "gemma3:4b"
    temperature: 0.7
    top_p: 0.95
    max_tokens: 2048

  lmstudio:
    base_url: "http://localhost:1234"
    model: "local-model"
    temperature: 0.7
    top_p: 0.95
    max_tokens: 2048