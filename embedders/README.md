# Embedders Module - Integrated Chunking + Embedding

## ğŸ¯ Tá»•ng quan

Module `embedders` cung cáº¥p há»‡ thá»‘ng embedding tÃ­ch há»£p vá»›i chunking, há»— trá»£ nhiá»u providers (Ollama, OpenAI, BGE) vÃ  cho phÃ©p chuyá»ƒn Ä‘á»•i model nhanh chÃ³ng.

**Kiáº¿n trÃºc chÃ­nh:**
PDF/Text â†’ PDFLoader â†’ PDFDocument â†’ HybridChunker â†’ ChunkSet â†’ ChunkSetEmbedder â†’ EmbeddingResults

## ğŸš€ CÃ¡ch sá»­ dá»¥ng nhanh

### 1. Pipeline tÃ­ch há»£p (Khuyáº¿n nghá»‹)

```python
from embedders import ChunkAndEmbedPipeline, EmbedderFactory

# Táº¡o embedder vá»›i Ollama
factory = EmbedderFactory()
embedder = factory.create_ollama_nomic()

# Táº¡o pipeline tÃ­ch há»£p
pipeline = ChunkAndEmbedPipeline(embedder=embedder)

# Xá»­ lÃ½ PDF end-to-end
results = pipeline.process_pdf("document.pdf")

# Hoáº·c xá»­ lÃ½ text
results = pipeline.process_text("Your text content here")
```

### 2. Chuyá»ƒn Ä‘á»•i model nhanh

```python
# Kiá»ƒm tra models available
available_models = pipeline.get_available_models()
print(f"Available models: {available_models}")

# Chuyá»ƒn model
pipeline.switch_embedder_model("all-MiniLM-L6-v2")
```

### 3. Sá»­ dá»¥ng riÃªng láº»

```python
from embedders import EmbedderFactory, EmbedderType, ChunkSetEmbedder
from chunkers import HybridChunker
from loaders import PDFLoader

# Load vÃ  chunk
loader = PDFLoader.create_default()
pdf_doc = loader.load("doc.pdf")
pdf_doc = pdf_doc.normalize()

chunker = HybridChunker()
chunk_set = chunker.chunk(pdf_doc)

# Embed
embedder = EmbedderFactory().create_ollama_nomic()
orchestrator = ChunkSetEmbedder(embedder)
results = orchestrator.embed_chunk_set(chunk_set)
```

## ğŸ”§ Providers há»— trá»£

### Ollama (Khuyáº¿n nghá»‹ cho local)

```python
from embedders import EmbedderFactory

factory = EmbedderFactory()

# Nomic embed (default)
embedder = factory.create_ollama_nomic()

# MiniLM
embedder = factory.create_ollama_minilm()

# Custom base URL
embedder = factory.create_ollama_nomic(base_url="http://192.168.1.100:11434")
```

**CÃ i Ä‘áº·t Ollama:**

```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Pull models
ollama pull nomic-embed-text
ollama pull all-MiniLM-L6-v2

# Start server
ollama serve
```

### OpenAI

```python
embedder = factory.create_openai_ada(api_key="your-api-key")
```

### BGE (HuggingFace)

```python
from embedders import EmbeddingProfile, EmbedderType

profile = EmbeddingProfile.create_bge_large()
embedder = factory.create(EmbedderType.BGE, profile)
```

## ğŸ“Š Pipeline Features

### ChunkAndEmbedPipeline

- **TÃ­ch há»£p end-to-end**: PDF â†’ Chunks â†’ Embeddings
- **Model switching**: Äá»•i model Ollama nhanh chÃ³ng
- **Parallel processing**: Xá»­ lÃ½ nhiá»u chunks song song
- **Metadata inclusion**: Bao gá»“m metadata trong embedding
- **Context awareness**: ThÃªm context tá»« chunks lÃ¢n cáº­n
- **Error handling**: Robust error handling vÃ  logging

### ChunkSetEmbedder

- **Batch processing**: Tá»‘i Æ°u cho nhiá»u chunks
- **Statistics**: BÃ¡o cÃ¡o chi tiáº¿t vá» quÃ¡ trÃ¬nh embedding
- **Provider agnostic**: Hoáº¡t Ä‘á»™ng vá»›i má»i embedder

## ğŸ—ï¸ Architecture

### Core Classes

IEmbedder (interface)
â”œâ”€â”€ BaseEmbedder (abstract base)
â”‚   â”œâ”€â”€ OllamaEmbedder
â”‚   â”œâ”€â”€ OpenAIEmbedder
â”‚   â””â”€â”€ BGEEmbedder

### Data Flow

1. **Input**: PDF file hoáº·c raw text
2. **Loading**: PDFLoader extract text/tables
3. **Normalization**: Clean vÃ  chuáº©n hÃ³a data
4. **Chunking**: HybridChunker táº¡o chunks
5. **Embedding**: Embedder táº¡o vectors
6. **Output**: Dict[chunk_id â†’ EmbeddingResult]

## âš™ï¸ Configuration

### EmbeddingProfile

```python
from embedders import EmbeddingProfile

# Ollama profiles
profile = EmbeddingProfile.create_ollama_nomic()
profile = EmbeddingProfile.create_ollama_minilm()

# Custom profile
profile = EmbeddingProfile(
    model_id="custom-model",
    provider="ollama",
    max_tokens=1024,
    dimension=512,
    normalize=True
)
```

### Pipeline Settings

```python
pipeline = ChunkAndEmbedPipeline(
    embedder=embedder,
    include_metadata=True,      # Include chunk metadata
    include_context=False,      # Add adjacent chunk context
    parallel_workers=2          # Parallel embedding workers
)
```

## ğŸ“ˆ Performance Tips

### Ollama Optimization

- Sá»­ dá»¥ng models nhá» hÆ¡n cho tá»‘c Ä‘á»™: `all-MiniLM-L6-v2`
- TÄƒng batch_size náº¿u cÃ³ GPU
- Monitor RAM usage vá»›i models lá»›n

### Parallel Processing

```python
# TÄƒng workers cho nhiá»u chunks
pipeline = ChunkAndEmbedPipeline(
    embedder=embedder,
    parallel_workers=4  # Sá»­ dá»¥ng 4 threads
)
```

### Memory Management

- Xá»­ lÃ½ files lá»›n tá»«ng pháº§n
- Clear results sau khi sá»­ dá»¥ng
- Monitor embedding dimensions

## ğŸ” Monitoring & Debugging

### Pipeline Info

```python
info = pipeline.get_pipeline_info()
print(f"Embedder: {info['embedder']['model']}")
print(f"Chunker: {info['chunker']['type']}")
```

### Embedding Statistics

```python
stats = orchestrator.get_embedding_stats(results)
print(f"Success rate: {stats['success_rate']:.1%}")
print(f"Avg processing time: {stats['avg_processing_time']:.3f}s")
```

### Error Handling

```python
for chunk_id, result in results.items():
    if result.status != "success":
        print(f"Failed {chunk_id}: {result.error_message}")
```

## ğŸ¯ Best Practices

1. **Chá»n model phÃ¹ há»£p**: Nomic cho cháº¥t lÆ°á»£ng, MiniLM cho tá»‘c Ä‘á»™
2. **Monitor resources**: Theo dÃµi CPU/Memory vá»›i models lá»›n
3. **Batch processing**: Sá»­ dá»¥ng parallel workers cho nhiá»u documents
4. **Error recovery**: Handle network issues vá»›i Ollama
5. **Caching**: Cache embeddings Ä‘á»ƒ trÃ¡nh re-processing

## ğŸš¨ Troubleshooting

### Ollama Issues

```bash
# Check Ollama status
curl http://localhost:11434/api/tags

# Restart Ollama
ollama serve

# Check logs
ollama logs
```

### Common Errors

- **Connection refused**: Ollama khÃ´ng cháº¡y
- **Model not found**: ChÆ°a pull model
- **Out of memory**: Giáº£m batch_size hoáº·c dÃ¹ng model nhá» hÆ¡n
- **Timeout**: TÄƒng timeout hoáº·c kiá»ƒm tra network

## ğŸ“ Examples

Xem `run_embed_demo.py` Ä‘á»ƒ cÃ³ vÃ­ dá»¥ Ä‘áº§y Ä‘á»§ vá» cÃ¡ch sá»­ dá»¥ng pipeline.
